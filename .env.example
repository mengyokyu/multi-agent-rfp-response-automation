# AI-Based RFP Automation System - Environment Variables


# ============================================================
# LLM CONFIGURATION
# ============================================================

# LLM Provider: 'groq' or 'ollama'
LLM_PROVIDER=ollama

# Groq API Configuration (if using Groq)
GROQ_API_KEY=your_groq_api_key_here
GROQ_MODEL=mixtral-8x7b-32768
# Available models: mixtral-8x7b-32768, llama2-70b-4096, gemma-7b-it

# Ollama Configuration (if using Ollama)
OLLAMA_MODEL=llama2
# Available models: llama2, mistral, codellama, neural-chat
OLLAMA_BASE_URL=http://localhost:11434

# LLM Temperature (0.0 - 1.0)
LLM_TEMPERATURE=0.3

# ============================================================
# APPLICATION CONFIGURATION
# ============================================================

# Server Configuration
PORT=8000
HOST=0.0.0.0

# Data Paths
OEM_CATALOG_PATH=data/oem_catalog.json
TEST_PRICING_PATH=data/test_pricing.json

# ============================================================
# SETUP INSTRUCTIONS
# ============================================================

# For Groq (Cloud, Fast):
# Set GROQ_API_KEY=your_key

# For Ollama (Local, Private):]
# Pull a model: ollama pull llama2]
